{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all necessary libraries\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "pd.set_option('max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's build a basic bag of words model on three sample documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gangs of Wasseypur is a great movie.', 'The success of a movie depends on the performance of the actors.', 'There are no new movies releasing this week.']\n"
     ]
    }
   ],
   "source": [
    "documents = [\"Gangs of Wasseypur is a great movie.\", \"The success of a movie depends on the performance of the actors.\", \"There are no new movies releasing this week.\"]\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gangs wasseypur great movie .', 'success movie depends performance actors .', 'new movies releasing week .']\n"
     ]
    }
   ],
   "source": [
    "def preprocess(document):\n",
    "    'changes document to lower case and removes stopwords'\n",
    "\n",
    "    # change sentence to lower case\n",
    "    document = document.lower()\n",
    "\n",
    "    # tokenize into words\n",
    "    words = word_tokenize(document)\n",
    "\n",
    "    # remove stop words\n",
    "    words = [word for word in words if word not in stopwords.words(\"english\")]\n",
    "\n",
    "    # join words to make sentence\n",
    "    document = \" \".join(words)\n",
    "    \n",
    "    return document\n",
    "\n",
    "documents = [preprocess(document) for document in documents]\n",
    "print(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating bag of words model using count vectorizer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 0)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 11)\t1\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "bow_model = vectorizer.fit_transform(documents)\n",
    "print(bow_model)  # returns the rown and column number of cells which have 1 as value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 1 0 0 0 0 0 1 0]\n",
      " [1 1 0 0 1 0 0 1 0 1 0 0]\n",
      " [0 0 0 0 0 1 1 0 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# print the full sparse matrix\n",
    "print(bow_model.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on CountVectorizer in module sklearn.feature_extraction.text object:\n",
      "\n",
      "class CountVectorizer(_VectorizerMixin, sklearn.base.BaseEstimator)\n",
      " |  CountVectorizer(*, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)\n",
      " |  \n",
      " |  Convert a collection of text documents to a matrix of token counts.\n",
      " |  \n",
      " |  This implementation produces a sparse representation of the counts using\n",
      " |  scipy.sparse.csr_matrix.\n",
      " |  \n",
      " |  If you do not provide an a-priori dictionary and you do not use an analyzer\n",
      " |  that does some kind of feature selection then the number of features will\n",
      " |  be equal to the vocabulary size found by analyzing the data.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  input : {'filename', 'file', 'content'}, default='content'\n",
      " |      - If `'filename'`, the sequence passed as an argument to fit is\n",
      " |        expected to be a list of filenames that need reading to fetch\n",
      " |        the raw content to analyze.\n",
      " |  \n",
      " |      - If `'file'`, the sequence items must have a 'read' method (file-like\n",
      " |        object) that is called to fetch the bytes in memory.\n",
      " |  \n",
      " |      - If `'content'`, the input is expected to be a sequence of items that\n",
      " |        can be of type string or byte.\n",
      " |  \n",
      " |  encoding : str, default='utf-8'\n",
      " |      If bytes or files are given to analyze, this encoding is used to\n",
      " |      decode.\n",
      " |  \n",
      " |  decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
      " |      Instruction on what to do if a byte sequence is given to analyze that\n",
      " |      contains characters not of the given `encoding`. By default, it is\n",
      " |      'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
      " |      values are 'ignore' and 'replace'.\n",
      " |  \n",
      " |  strip_accents : {'ascii', 'unicode'} or callable, default=None\n",
      " |      Remove accents and perform other character normalization\n",
      " |      during the preprocessing step.\n",
      " |      'ascii' is a fast method that only works on characters that have\n",
      " |      a direct ASCII mapping.\n",
      " |      'unicode' is a slightly slower method that works on any characters.\n",
      " |      None (default) does nothing.\n",
      " |  \n",
      " |      Both 'ascii' and 'unicode' use NFKD normalization from\n",
      " |      :func:`unicodedata.normalize`.\n",
      " |  \n",
      " |  lowercase : bool, default=True\n",
      " |      Convert all characters to lowercase before tokenizing.\n",
      " |  \n",
      " |  preprocessor : callable, default=None\n",
      " |      Override the preprocessing (strip_accents and lowercase) stage while\n",
      " |      preserving the tokenizing and n-grams generation steps.\n",
      " |      Only applies if ``analyzer`` is not callable.\n",
      " |  \n",
      " |  tokenizer : callable, default=None\n",
      " |      Override the string tokenization step while preserving the\n",
      " |      preprocessing and n-grams generation steps.\n",
      " |      Only applies if ``analyzer == 'word'``.\n",
      " |  \n",
      " |  stop_words : {'english'}, list, default=None\n",
      " |      If 'english', a built-in stop word list for English is used.\n",
      " |      There are several known issues with 'english' and you should\n",
      " |      consider an alternative (see :ref:`stop_words`).\n",
      " |  \n",
      " |      If a list, that list is assumed to contain stop words, all of which\n",
      " |      will be removed from the resulting tokens.\n",
      " |      Only applies if ``analyzer == 'word'``.\n",
      " |  \n",
      " |      If None, no stop words will be used. In this case, setting `max_df`\n",
      " |      to a higher value, such as in the range (0.7, 1.0), can automatically detect\n",
      " |      and filter stop words based on intra corpus document frequency of terms.\n",
      " |  \n",
      " |  token_pattern : str or None, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n",
      " |      Regular expression denoting what constitutes a \"token\", only used\n",
      " |      if ``analyzer == 'word'``. The default regexp select tokens of 2\n",
      " |      or more alphanumeric characters (punctuation is completely ignored\n",
      " |      and always treated as a token separator).\n",
      " |  \n",
      " |      If there is a capturing group in token_pattern then the\n",
      " |      captured group content, not the entire match, becomes the token.\n",
      " |      At most one capturing group is permitted.\n",
      " |  \n",
      " |  ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
      " |      The lower and upper boundary of the range of n-values for different\n",
      " |      word n-grams or char n-grams to be extracted. All values of n such\n",
      " |      such that min_n <= n <= max_n will be used. For example an\n",
      " |      ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n",
      " |      unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n",
      " |      Only applies if ``analyzer`` is not callable.\n",
      " |  \n",
      " |  analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n",
      " |      Whether the feature should be made of word n-gram or character\n",
      " |      n-grams.\n",
      " |      Option 'char_wb' creates character n-grams only from text inside\n",
      " |      word boundaries; n-grams at the edges of words are padded with space.\n",
      " |  \n",
      " |      If a callable is passed it is used to extract the sequence of features\n",
      " |      out of the raw, unprocessed input.\n",
      " |  \n",
      " |      .. versionchanged:: 0.21\n",
      " |  \n",
      " |      Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
      " |      first read from the file and then passed to the given callable\n",
      " |      analyzer.\n",
      " |  \n",
      " |  max_df : float in range [0.0, 1.0] or int, default=1.0\n",
      " |      When building the vocabulary ignore terms that have a document\n",
      " |      frequency strictly higher than the given threshold (corpus-specific\n",
      " |      stop words).\n",
      " |      If float, the parameter represents a proportion of documents, integer\n",
      " |      absolute counts.\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  min_df : float in range [0.0, 1.0] or int, default=1\n",
      " |      When building the vocabulary ignore terms that have a document\n",
      " |      frequency strictly lower than the given threshold. This value is also\n",
      " |      called cut-off in the literature.\n",
      " |      If float, the parameter represents a proportion of documents, integer\n",
      " |      absolute counts.\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  max_features : int, default=None\n",
      " |      If not None, build a vocabulary that only consider the top\n",
      " |      `max_features` ordered by term frequency across the corpus.\n",
      " |      Otherwise, all features are used.\n",
      " |  \n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  vocabulary : Mapping or iterable, default=None\n",
      " |      Either a Mapping (e.g., a dict) where keys are terms and values are\n",
      " |      indices in the feature matrix, or an iterable over terms. If not\n",
      " |      given, a vocabulary is determined from the input documents. Indices\n",
      " |      in the mapping should not be repeated and should not have any gap\n",
      " |      between 0 and the largest index.\n",
      " |  \n",
      " |  binary : bool, default=False\n",
      " |      If True, all non zero counts are set to 1. This is useful for discrete\n",
      " |      probabilistic models that model binary events rather than integer\n",
      " |      counts.\n",
      " |  \n",
      " |  dtype : dtype, default=np.int64\n",
      " |      Type of the matrix returned by fit_transform() or transform().\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  vocabulary_ : dict\n",
      " |      A mapping of terms to feature indices.\n",
      " |  \n",
      " |  fixed_vocabulary_ : bool\n",
      " |      True if a fixed vocabulary of term to indices mapping\n",
      " |      is provided by the user.\n",
      " |  \n",
      " |  stop_words_ : set\n",
      " |      Terms that were ignored because they either:\n",
      " |  \n",
      " |        - occurred in too many documents (`max_df`)\n",
      " |        - occurred in too few documents (`min_df`)\n",
      " |        - were cut off by feature selection (`max_features`).\n",
      " |  \n",
      " |      This is only available if no vocabulary was given.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  HashingVectorizer : Convert a collection of text documents to a\n",
      " |      matrix of token counts.\n",
      " |  \n",
      " |  TfidfVectorizer : Convert a collection of raw documents to a matrix\n",
      " |      of TF-IDF features.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The ``stop_words_`` attribute can get large and increase the model size\n",
      " |  when pickling. This attribute is provided only for introspection and can\n",
      " |  be safely removed using delattr or set to None before pickling.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.feature_extraction.text import CountVectorizer\n",
      " |  >>> corpus = [\n",
      " |  ...     'This is the first document.',\n",
      " |  ...     'This document is the second document.',\n",
      " |  ...     'And this is the third one.',\n",
      " |  ...     'Is this the first document?',\n",
      " |  ... ]\n",
      " |  >>> vectorizer = CountVectorizer()\n",
      " |  >>> X = vectorizer.fit_transform(corpus)\n",
      " |  >>> vectorizer.get_feature_names_out()\n",
      " |  array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
      " |         'this'], ...)\n",
      " |  >>> print(X.toarray())\n",
      " |  [[0 1 1 1 0 0 1 0 1]\n",
      " |   [0 2 0 1 0 1 1 0 1]\n",
      " |   [1 0 0 1 1 0 1 1 1]\n",
      " |   [0 1 1 1 0 0 1 0 1]]\n",
      " |  >>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
      " |  >>> X2 = vectorizer2.fit_transform(corpus)\n",
      " |  >>> vectorizer2.get_feature_names_out()\n",
      " |  array(['and this', 'document is', 'first document', 'is the', 'is this',\n",
      " |         'second document', 'the first', 'the second', 'the third', 'third one',\n",
      " |         'this document', 'this is', 'this the'], ...)\n",
      " |   >>> print(X2.toarray())\n",
      " |   [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
      " |   [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
      " |   [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
      " |   [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CountVectorizer\n",
      " |      _VectorizerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.utils._metadata_requests._MetadataRequester\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, raw_documents, y=None)\n",
      " |      Learn a vocabulary dictionary of all tokens in the raw documents.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which generates either str, unicode or file objects.\n",
      " |      \n",
      " |      y : None\n",
      " |          This parameter is ignored.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted vectorizer.\n",
      " |  \n",
      " |  fit_transform(self, raw_documents, y=None)\n",
      " |      Learn the vocabulary dictionary and return document-term matrix.\n",
      " |      \n",
      " |      This is equivalent to fit followed by transform, but more efficiently\n",
      " |      implemented.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which generates either str, unicode or file objects.\n",
      " |      \n",
      " |      y : None\n",
      " |          This parameter is ignored.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : array of shape (n_samples, n_features)\n",
      " |          Document-term matrix.\n",
      " |  \n",
      " |  get_feature_names_out(self, input_features=None)\n",
      " |      Get output feature names for transformation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      input_features : array-like of str or None, default=None\n",
      " |          Not used, present here for API consistency by convention.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_names_out : ndarray of str objects\n",
      " |          Transformed feature names.\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Return terms per document with nonzero entries in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Document-term matrix.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_inv : list of arrays of shape (n_samples,)\n",
      " |          List of arrays of terms.\n",
      " |  \n",
      " |  set_fit_request(self: sklearn.feature_extraction.text.CountVectorizer, *, raw_documents: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.feature_extraction.text.CountVectorizer\n",
      " |      Request metadata passed to the ``fit`` method.\n",
      " |      \n",
      " |      Note that this method is only relevant if\n",
      " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      The options for each parameter are:\n",
      " |      \n",
      " |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      " |      \n",
      " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      " |      \n",
      " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      " |      \n",
      " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      " |      \n",
      " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      " |      existing request. This allows you to change the request for some\n",
      " |      parameters and not others.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method is only relevant if this estimator is used as a\n",
      " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      " |          :class:`pipeline.Pipeline`. Otherwise it has no effect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``raw_documents`` parameter in ``fit``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          The updated object.\n",
      " |  \n",
      " |  set_transform_request(self: sklearn.feature_extraction.text.CountVectorizer, *, raw_documents: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.feature_extraction.text.CountVectorizer\n",
      " |      Request metadata passed to the ``transform`` method.\n",
      " |      \n",
      " |      Note that this method is only relevant if\n",
      " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      The options for each parameter are:\n",
      " |      \n",
      " |      - ``True``: metadata is requested, and passed to ``transform`` if provided. The request is ignored if metadata is not provided.\n",
      " |      \n",
      " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``transform``.\n",
      " |      \n",
      " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      " |      \n",
      " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      " |      \n",
      " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      " |      existing request. This allows you to change the request for some\n",
      " |      parameters and not others.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method is only relevant if this estimator is used as a\n",
      " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      " |          :class:`pipeline.Pipeline`. Otherwise it has no effect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``raw_documents`` parameter in ``transform``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          The updated object.\n",
      " |  \n",
      " |  transform(self, raw_documents)\n",
      " |      Transform documents to document-term matrix.\n",
      " |      \n",
      " |      Extract token counts out of raw text documents using the vocabulary\n",
      " |      fitted with fit or the one provided to the constructor.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which generates either str, unicode or file objects.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : sparse matrix of shape (n_samples, n_features)\n",
      " |          Document-term matrix.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _VectorizerMixin:\n",
      " |  \n",
      " |  build_analyzer(self)\n",
      " |      Return a callable to process input data.\n",
      " |      \n",
      " |      The callable handles preprocessing, tokenization, and n-grams generation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      analyzer: callable\n",
      " |          A function to handle preprocessing, tokenization\n",
      " |          and n-grams generation.\n",
      " |  \n",
      " |  build_preprocessor(self)\n",
      " |      Return a function to preprocess the text before tokenization.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      preprocessor: callable\n",
      " |            A function to preprocess the text before tokenization.\n",
      " |  \n",
      " |  build_tokenizer(self)\n",
      " |      Return a function that splits a string into a sequence of tokens.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      tokenizer: callable\n",
      " |            A function to split a string into a sequence of tokens.\n",
      " |  \n",
      " |  decode(self, doc)\n",
      " |      Decode the input into a string of unicode symbols.\n",
      " |      \n",
      " |      The decoding strategy depends on the vectorizer parameters.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      doc : bytes or str\n",
      " |          The string to decode.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      doc: str\n",
      " |          A string of unicode symbols.\n",
      " |  \n",
      " |  get_stop_words(self)\n",
      " |      Build or fetch the effective stop words list.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      stop_words: list or None\n",
      " |              A list of stop words.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from _VectorizerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  __sklearn_clone__(self)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |  \n",
      " |  get_metadata_routing(self)\n",
      " |      Get metadata routing of this object.\n",
      " |      \n",
      " |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      routing : MetadataRequest\n",
      " |          A :class:`~utils.metadata_routing.MetadataRequest` encapsulating\n",
      " |          routing information.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |  \n",
      " |  __init_subclass__(**kwargs) from builtins.type\n",
      " |      Set the ``set_{method}_request`` methods.\n",
      " |      \n",
      " |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      " |      looks for the information available in the set default values which are\n",
      " |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      " |      from method signatures.\n",
      " |      \n",
      " |      The ``__metadata_request__*`` class attributes are used when a method\n",
      " |      does not explicitly accept a metadata through its arguments or if the\n",
      " |      developer would like to specify a request value for those metadata\n",
      " |      which are different from the default ``None``.\n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 12)\n",
      "['actors' 'depends' 'gangs' 'great' 'movie' 'movies' 'new' 'performance'\n",
      " 'releasing' 'success' 'wasseypur' 'week']\n"
     ]
    }
   ],
   "source": [
    "print(bow_model.shape)\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create a bag of words model on the spam dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                               message  \n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...  \n",
       "1                                                                        Ok lar... Joking wif u oni...  \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
       "3                                                    U dun say so early hor... U c already then say...  \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "spam = pd.read_csv(\"002_SMSSpamCollection.txt\", sep = \"\\t\", names=[\"label\", \"message\"])\n",
    "spam.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's take a subset of data (first 50 rows only) and create bag of word model on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label  \\\n",
      "0    ham   \n",
      "1    ham   \n",
      "2   spam   \n",
      "3    ham   \n",
      "4    ham   \n",
      "..   ...   \n",
      "95  spam   \n",
      "96   ham   \n",
      "97   ham   \n",
      "98   ham   \n",
      "99   ham   \n",
      "\n",
      "                                                                                                message  \n",
      "0   Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...  \n",
      "1                                                                         Ok lar... Joking wif u oni...  \n",
      "2   Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
      "3                                                     U dun say so early hor... U c already then say...  \n",
      "4                                         Nah I don't think he goes to usf, he lives around here though  \n",
      "..                                                                                                  ...  \n",
      "95  Your free ringtone is waiting to be collected. Simply text the password \"MIX\" to 85069 to verify...  \n",
      "96                                                                    Watching telugu movie..wat abt u?  \n",
      "97                                                  i see. When we finish we have loads of loans to pay  \n",
      "98  Hi. Wk been ok - on hols now! Yes on for a bit of a run. Forgot that i have hairdressers appoint...  \n",
      "99                                                                      I see a cup of coffee animation  \n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "spam = spam.iloc[0:100,:]\n",
    "print(spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...\n",
      "1                                                                           Ok lar... Joking wif u oni...\n",
      "2     Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...\n",
      "3                                                       U dun say so early hor... U c already then say...\n",
      "4                                           Nah I don't think he goes to usf, he lives around here though\n",
      "                                                     ...                                                 \n",
      "95    Your free ringtone is waiting to be collected. Simply text the password \"MIX\" to 85069 to verify...\n",
      "96                                                                      Watching telugu movie..wat abt u?\n",
      "97                                                    i see. When we finish we have loads of loans to pay\n",
      "98    Hi. Wk been ok - on hols now! Yes on for a bit of a run. Forgot that i have hairdressers appoint...\n",
      "99                                                                        I see a cup of coffee animation\n",
      "Name: message, Length: 100, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# extract the messages from the dataframe\n",
    "messages = spam.message\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...', 'Ok lar... Joking wif u oni...', \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\", 'U dun say so early hor... U c already then say...', \"Nah I don't think he goes to usf, he lives around here though\", \"FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\", 'Even my brother is not like to speak with me. They treat me like aids patent.', \"As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\", 'WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.', 'Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030', \"I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.\", 'SIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info', 'URGENT! You have won a 1 week FREE membership in our £100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18', \"I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.\", 'I HAVE A DATE ON SUNDAY WITH WILL!!', 'XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here>> http://wap. xxxmobilemovieclub.com?n=QJKGIGHJJGCBL', \"Oh k...i'm watching here:)\", 'Eh u remember how 2 spell his name... Yes i did. He v naughty make until i v wet.', 'Fine if that\\x92s the way u feel. That\\x92s the way its gota b', 'England v Macedonia - dont miss the goals/team news. Txt ur national team to 87077 eg ENGLAND to 87077 Try:WALES, SCOTLAND 4txt/ú1.20 POBOXox36504W45WQ 16+', 'Is that seriously how you spell his name?', 'I‘m going to try for 2 months ha ha only joking', 'So ü pay first lar... Then when is da stock comin...', 'Aft i finish my lunch then i go str down lor. Ard 3 smth lor. U finish ur lunch already?', 'Ffffffffff. Alright no way I can meet up with you sooner?', \"Just forced myself to eat a slice. I'm really not hungry tho. This sucks. Mark is getting worried. He knows I'm sick when I turn down pizza. Lol\", 'Lol your always so convincing.', \"Did you catch the bus ? Are you frying an egg ? Did you make a tea? Are you eating your mom's left over dinner ? Do you feel my Love ?\", \"I'm back &amp; we're packing the car now, I'll let you know if there's room\", 'Ahhh. Work. I vaguely remember that! What does it feel like? Lol', \"Wait that's still not all that clear, were you not sure about me being sarcastic or that that's why x doesn't want to live with us\", \"Yeah he got in at 2 and was v apologetic. n had fallen out and she was actin like spoilt child and he got caught up in that. Till 2! But we won't go there! Not doing too badly cheers. You? \", 'K tell me anything about you.', 'For fear of fainting with the of all that housework you just did? Quick have a cuppa', 'Thanks for your subscription to Ringtone UK your mobile will be charged £5/month Please confirm by replying YES or NO. If you reply NO you will not be charged', 'Yup... Ok i go home look at the timings then i msg ü again... Xuhui going to learn on 2nd may too but her lesson is at 8am', \"Oops, I'll let you know when my roommate's done\", 'I see the letter B on my car', 'Anything lor... U decide...', \"Hello! How's you and how did saturday go? I was just texting to see if you'd decided to do anything tomo. Not that i'm trying to invite myself or anything!\", 'Pls go ahead with watts. I just wanted to be sure. Do have a great weekend. Abiola', 'Did I forget to tell you ? I want you , I need you, I crave you ... But most of all ... I love you my sweet Arabian steed ... Mmmmmm ... Yummy', '07732584351 - Rodger Burns - MSG = We tried to call you re your reply to our sms for a free nokia mobile + free camcorder. Please call now 08000930705 for delivery tomorrow', 'WHO ARE YOU SEEING?', 'Great! I hope you like your man well endowed. I am  &lt;#&gt;  inches...', 'No calls..messages..missed calls', \"Didn't you get hep b immunisation in nigeria.\", 'Fair enough, anything going on?', \"Yeah hopefully, if tyler can't do it I could maybe ask around a bit\", \"U don't know how stubborn I am. I didn't even want to go to the hospital. I kept telling Mark I'm not a weak sucker. Hospitals are for weak suckers.\", 'What you thinked about me. First time you saw me in class.', 'A gram usually runs like  &lt;#&gt; , a half eighth is smarter though and gets you almost a whole second gram for  &lt;#&gt;', \"K fyi x has a ride early tomorrow morning but he's crashing at our place tonight\", 'Wow. I never realized that you were so embarassed by your accomodations. I thought you liked it, since i was doing the best i could and you always seemed so happy about \"the cave\". I\\'m sorry I didn\\'t and don\\'t have more to give. I\\'m sorry i offered. I\\'m sorry your room was so embarassing.', 'SMS. ac Sptv: The New Jersey Devils and the Detroit Red Wings play Ice Hockey. Correct or Incorrect? End? Reply END SPTV', 'Do you know what Mallika Sherawat did yesterday? Find out now @  &lt;URL&gt;', 'Congrats! 1 year special cinema pass for 2 is yours. call 09061209465 now! C Suprman V, Matrix3, StarWars3, etc all 4 FREE! bx420-ip4-5we. 150pm. Dont miss out! ', \"Sorry, I'll call later in meeting.\", 'Tell where you reached', 'Yes..gauti and sehwag out of odi series.', \"Your gonna have to pick up a $1 burger for yourself on your way home. I can't even move. Pain is killing me.\", 'Ha ha ha good joke. Girls are situation seekers.', 'Its a part of checking IQ', 'Sorry my roommates took forever, it ok if I come by now?', 'Ok lar i double check wif da hair dresser already he said wun cut v short. He said will cut until i look nice.', 'As a valued customer, I am pleased to advise you that following recent review of your Mob No. you are awarded with a £1500 Bonus Prize, call 09066364589', 'Today is \"song dedicated day..\" Which song will u dedicate for me? Send this to all ur valuable frnds but first rply me...', 'Urgent UR awarded a complimentary trip to EuroDisinc Trav, Aco&Entry41 Or £1000. To claim txt DIS to 87121 18+6*£1.50(moreFrmMob. ShrAcomOrSglSuplt)10, LS1 3AJ', 'Did you hear about the new \"Divorce Barbie\"? It comes with all of Ken\\'s stuff!', 'I plane to give on this month end.', 'Wah lucky man... Then can save money... Hee...', 'Finished class where are you.', 'HI BABE IM AT HOME NOW WANNA DO SOMETHING? XX', 'K..k:)where are you?how did you performed?', 'U can call me now...', 'I am waiting machan. Call me once you free.', 'Thats cool. i am a gentleman and will treat you with dignity and respect.', 'I like you peoples very much:) but am very shy pa.', 'Does not operate after  &lt;#&gt;  or what', \"Its not the same here. Still looking for a job. How much do Ta's earn there.\", \"Sorry, I'll call later\", 'K. Did you call me just now ah? ', 'Ok i am on the way to home hi hi', 'You will be in the place of that man', 'Yup next stop.', \"I call you later, don't have network. If urgnt, sms me.\", \"For real when u getting on yo? I only need 2 more tickets and one more jacket and I'm done. I already used all my multis.\", \"Yes I started to send requests to make it but pain came back so I'm back in bed. Double coins at the factory too. I gotta cash in all my nitros.\", \"I'm really not up to it still tonight babe\", 'Ela kano.,il download, come wen ur free..', 'Yeah do! Don‘t stand to close tho- you‘ll catch something!', \"Sorry to be a pain. Is it ok if we meet another night? I spent late afternoon in casualty and that means i haven't done any of y stuff42moro and that includes all my time sheets and that. Sorry. \", 'Smile in Pleasure Smile in Pain Smile when trouble pours like Rain Smile when sum1 Hurts U Smile becoz SOMEONE still Loves to see u Smiling!!', 'Please call our customer service representative on 0800 169 6031 between 10am-9pm as you have WON a guaranteed £1000 cash or £5000 prize!', 'Havent planning to buy later. I check already lido only got 530 show in e afternoon. U finish work already?', 'Your free ringtone is waiting to be collected. Simply text the password \"MIX\" to 85069 to verify. Get Usher and Britney. FML, PO Box 5249, MK17 92H. 450Ppw 16', 'Watching telugu movie..wat abt u?', 'i see. When we finish we have loads of loans to pay', 'Hi. Wk been ok - on hols now! Yes on for a bit of a run. Forgot that i have hairdressers appointment at four so need to get home n shower beforehand. Does that cause prob for u?\"', 'I see a cup of coffee animation']\n"
     ]
    }
   ],
   "source": [
    "# convert messages into list\n",
    "messages = [message for message in messages]\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['go jurong point , crazy .. available bugis n great world la e buffet ... cine got amore wat ...', 'ok lar ... joking wif u oni ...', \"free entry 2 wkly comp win fa cup final tkts 21st may 2005. text fa 87121 receive entry question ( std txt rate ) & c 's apply 08452810075over18 's\", 'u dun say early hor ... u c already say ...', \"nah n't think goes usf , lives around though\", \"freemsg hey darling 's 3 week 's word back ! 'd like fun still ? tb ok ! xxx std chgs send , £1.50 rcv\", 'even brother like speak . treat like aids patent .', \"per request 'melle melle ( oru minnaminunginte nurungu vettam ) ' set callertune callers . press * 9 copy friends callertune\", 'winner ! ! valued network customer selected receivea £900 prize reward ! claim call 09061701461. claim code kl341 . valid 12 hours .', 'mobile 11 months ? u r entitled update latest colour mobiles camera free ! call mobile update co free 08002986030', \"'m gon na home soon n't want talk stuff anymore tonight , k ? 've cried enough today .\", 'six chances win cash ! 100 20,000 pounds txt > csh11 send 87575. cost 150p/day , 6days , 16+ tsandcs apply reply hl 4 info', 'urgent ! 1 week free membership £100,000 prize jackpot ! txt word : claim : 81010 & c www.dbuk.net lccltd pobox 4403ldnw1a7rw18', \"'ve searching right words thank breather . promise wont take help granted fulfil promise . wonderful blessing times .\", 'date sunday ! !', 'xxxmobilemovieclub : use credit , click wap link next txt message click > > http : //wap . xxxmobilemovieclub.com ? n=qjkgighjjgcbl', \"oh k ... 'm watching : )\", 'eh u remember 2 spell name ... yes . v naughty make v wet .', 'fine that\\x92s way u feel . that\\x92s way gota b', 'england v macedonia - dont miss goals/team news . txt ur national team 87077 eg england 87077 try : wales , scotland 4txt/ú1.20 poboxox36504w45wq 16+', 'seriously spell name ?', '‘ going try 2 months ha ha joking', 'ü pay first lar ... da stock comin ...', 'aft finish lunch go str lor . ard 3 smth lor . u finish ur lunch already ?', 'ffffffffff . alright way meet sooner ?', \"forced eat slice . 'm really hungry tho . sucks . mark getting worried . knows 'm sick turn pizza . lol\", 'lol always convincing .', \"catch bus ? frying egg ? make tea ? eating mom 's left dinner ? feel love ?\", \"'m back & amp ; 're packing car , 'll let know 's room\", 'ahhh . work . vaguely remember ! feel like ? lol', \"wait 's still clear , sure sarcastic 's x n't want live us\", \"yeah got 2 v apologetic . n fallen actin like spoilt child got caught . till 2 ! wo n't go ! badly cheers . ?\", 'k tell anything .', 'fear fainting housework ? quick cuppa', 'thanks subscription ringtone uk mobile charged £5/month please confirm replying yes . reply charged', 'yup ... ok go home look timings msg ü ... xuhui going learn 2nd may lesson 8am', \"oops , 'll let know roommate 's done\", 'see letter b car', 'anything lor ... u decide ...', \"hello ! 's saturday go ? texting see 'd decided anything tomo . 'm trying invite anything !\", 'pls go ahead watts . wanted sure . great weekend . abiola', 'forget tell ? want , need , crave ... ... love sweet arabian steed ... mmmmmm ... yummy', '07732584351 - rodger burns - msg = tried call reply sms free nokia mobile + free camcorder . please call 08000930705 delivery tomorrow', 'seeing ?', 'great ! hope like man well endowed . & lt ; # & gt ; inches ...', 'calls .. messages .. missed calls', \"n't get hep b immunisation nigeria .\", 'fair enough , anything going ?', \"yeah hopefully , tyler ca n't could maybe ask around bit\", \"u n't know stubborn . n't even want go hospital . kept telling mark 'm weak sucker . hospitals weak suckers .\", 'thinked . first time saw class .', 'gram usually runs like & lt ; # & gt ; , half eighth smarter though gets almost whole second gram & lt ; # & gt ;', \"k fyi x ride early tomorrow morning 's crashing place tonight\", \"wow . never realized embarassed accomodations . thought liked , since best could always seemed happy `` cave '' . 'm sorry n't n't give . 'm sorry offered . 'm sorry room embarassing .\", 'sms . ac sptv : new jersey devils detroit red wings play ice hockey . correct incorrect ? end ? reply end sptv', 'know mallika sherawat yesterday ? find @ & lt ; url & gt ;', 'congrats ! 1 year special cinema pass 2 . call 09061209465 ! c suprman v , matrix3 , starwars3 , etc 4 free ! bx420-ip4-5we . 150pm . dont miss !', \"sorry , 'll call later meeting .\", 'tell reached', 'yes .. gauti sehwag odi series .', \"gon na pick $ 1 burger way home . ca n't even move . pain killing .\", 'ha ha ha good joke . girls situation seekers .', 'part checking iq', 'sorry roommates took forever , ok come ?', 'ok lar double check wif da hair dresser already said wun cut v short . said cut look nice .', 'valued customer , pleased advise following recent review mob . awarded £1500 bonus prize , call 09066364589', \"today `` song dedicated day .. '' song u dedicate ? send ur valuable frnds first rply ...\", 'urgent ur awarded complimentary trip eurodisinc trav , aco & entry41 £1000 . claim txt dis 87121 18+6 * £1.50 ( morefrmmob . shracomorsglsuplt ) 10 , ls1 3aj', \"hear new `` divorce barbie '' ? comes ken 's stuff !\", 'plane give month end .', 'wah lucky man ... save money ... hee ...', 'finished class .', 'hi babe im home wan na something ? xx', 'k .. k : ) ? performed ?', 'u call ...', 'waiting machan . call free .', 'thats cool . gentleman treat dignity respect .', 'like peoples much : ) shy pa .', 'operate & lt ; # & gt ;', \". still looking job . much ta 's earn .\", \"sorry , 'll call later\", 'k. call ah ?', 'ok way home hi hi', 'place man', 'yup next stop .', \"call later , n't network . urgnt , sms .\", \"real u getting yo ? need 2 tickets one jacket 'm done . already used multis .\", \"yes started send requests make pain came back 'm back bed . double coins factory . got ta cash nitros .\", \"'m really still tonight babe\", 'ela kano. , il download , come wen ur free ..', 'yeah ! ‘ stand close tho- ‘ catch something !', \"sorry pain . ok meet another night ? spent late afternoon casualty means n't done stuff42moro includes time sheets . sorry .\", 'smile pleasure smile pain smile trouble pours like rain smile sum1 hurts u smile becoz someone still loves see u smiling ! !', 'please call customer service representative 0800 169 6031 10am-9pm guaranteed £1000 cash £5000 prize !', 'havent planning buy later . check already lido got 530 show e afternoon . u finish work already ?', \"free ringtone waiting collected . simply text password `` mix '' 85069 verify . get usher britney . fml , po box 5249 , mk17 92h . 450ppw 16\", 'watching telugu movie .. wat abt u ?', 'see . finish loads loans pay', \"hi . wk ok - hols ! yes bit run . forgot hairdressers appointment four need get home n shower beforehand . cause prob u ? ''\", 'see cup coffee animation']\n"
     ]
    }
   ],
   "source": [
    "# preprocess messages using the preprocess function\n",
    "messages = [preprocess(message) for message in messages]\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of words model\n",
    "vectorizer = CountVectorizer()\n",
    "bow_model = vectorizer.fit_transform(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 640)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the dataframed\n",
    "df = pd.DataFrame(bow_model.toarray(), columns = vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000' '07732584351' '0800' '08000930705' '08002986030'\n",
      " '08452810075over18' '09061209465' '09061701461' '09066364589' '10' '100'\n",
      " '1000' '10am' '11' '12' '1500' '150p' '150pm' '16' '169' '18' '20' '2005'\n",
      " '21st' '2nd' '3aj' '4403ldnw1a7rw18' '450ppw' '4txt' '50' '5000' '5249'\n",
      " '530' '5we' '6031' '6days' '81010' '85069' '87077' '87121' '87575' '8am'\n",
      " '900' '92h' '9pm' 'abiola' 'abt' 'ac' 'accomodations' 'aco' 'actin'\n",
      " 'advise' 'aft' 'afternoon' 'ah' 'ahead' 'ahhh' 'aids' 'almost' 'already'\n",
      " 'alright' 'always' 'amore' 'amp' 'animation' 'another' 'anymore'\n",
      " 'anything' 'apologetic' 'apply' 'appointment' 'arabian' 'ard' 'around'\n",
      " 'ask' 'available' 'awarded' 'babe' 'back' 'badly' 'barbie' 'becoz' 'bed'\n",
      " 'beforehand' 'best' 'bit' 'blessing' 'bonus' 'box' 'breather' 'britney'\n",
      " 'brother' 'buffet' 'bugis' 'burger' 'burns' 'bus' 'buy' 'bx420' 'ca'\n",
      " 'call' 'callers' 'callertune' 'calls' 'camcorder' 'came' 'camera' 'car'\n",
      " 'cash' 'casualty' 'catch' 'caught' 'cause' 'cave' 'chances' 'charged'\n",
      " 'check' 'checking' 'cheers' 'chgs' 'child' 'cine' 'cinema' 'claim'\n",
      " 'class' 'clear' 'click' 'close' 'co' 'code' 'coffee' 'coins' 'collected'\n",
      " 'colour' 'com' 'come' 'comes' 'comin' 'comp' 'complimentary' 'confirm'\n",
      " 'congrats' 'convincing' 'cool' 'copy' 'correct' 'cost' 'could' 'crashing'\n",
      " 'crave' 'crazy' 'credit' 'cried' 'csh11' 'cup' 'cuppa' 'customer' 'cut'\n",
      " 'da' 'darling' 'date' 'day' 'dbuk' 'decide' 'decided' 'dedicate'\n",
      " 'dedicated' 'delivery' 'detroit' 'devils' 'dignity' 'dinner' 'dis'\n",
      " 'divorce' 'done' 'dont' 'double' 'download' 'dresser' 'dun' 'early'\n",
      " 'earn' 'eat' 'eating' 'eg' 'egg' 'eh' 'eighth' 'ela' 'embarassed'\n",
      " 'embarassing' 'end' 'endowed' 'england' 'enough' 'entitled' 'entry'\n",
      " 'entry41' 'etc' 'eurodisinc' 'even' 'fa' 'factory' 'fainting' 'fair'\n",
      " 'fallen' 'fear' 'feel' 'ffffffffff' 'final' 'find' 'fine' 'finish'\n",
      " 'finished' 'first' 'fml' 'following' 'forced' 'forever' 'forget' 'forgot'\n",
      " 'four' 'free' 'freemsg' 'friends' 'frnds' 'frying' 'fulfil' 'fun' 'fyi'\n",
      " 'gauti' 'gentleman' 'get' 'gets' 'getting' 'girls' 'give' 'go' 'goals'\n",
      " 'goes' 'going' 'gon' 'good' 'got' 'gota' 'gram' 'granted' 'great' 'gt'\n",
      " 'guaranteed' 'ha' 'hair' 'hairdressers' 'half' 'happy' 'havent' 'hear'\n",
      " 'hee' 'hello' 'help' 'hep' 'hey' 'hi' 'hl' 'hockey' 'hols' 'home' 'hope'\n",
      " 'hopefully' 'hor' 'hospital' 'hospitals' 'hours' 'housework' 'http'\n",
      " 'hungry' 'hurts' 'ice' 'il' 'im' 'immunisation' 'inches' 'includes'\n",
      " 'incorrect' 'info' 'invite' 'ip4' 'iq' 'jacket' 'jackpot' 'jersey' 'job'\n",
      " 'joke' 'joking' 'jurong' 'kano' 'ken' 'kept' 'killing' 'kl341' 'know'\n",
      " 'knows' 'la' 'lar' 'late' 'later' 'latest' 'lccltd' 'learn' 'left'\n",
      " 'lesson' 'let' 'letter' 'lido' 'like' 'liked' 'link' 'live' 'lives' 'll'\n",
      " 'loads' 'loans' 'lol' 'look' 'looking' 'lor' 'love' 'loves' 'ls1' 'lt'\n",
      " 'lucky' 'lunch' 'macedonia' 'machan' 'make' 'mallika' 'man' 'mark'\n",
      " 'matrix3' 'may' 'maybe' 'means' 'meet' 'meeting' 'melle' 'membership'\n",
      " 'message' 'messages' 'minnaminunginte' 'miss' 'missed' 'mix' 'mk17'\n",
      " 'mmmmmm' 'mob' 'mobile' 'mobiles' 'mom' 'money' 'month' 'months'\n",
      " 'morefrmmob' 'morning' 'move' 'movie' 'msg' 'much' 'multis' 'na' 'nah'\n",
      " 'name' 'national' 'naughty' 'need' 'net' 'network' 'never' 'new' 'news'\n",
      " 'next' 'nice' 'nigeria' 'night' 'nitros' 'nokia' 'nurungu' 'odi'\n",
      " 'offered' 'oh' 'ok' 'one' 'oni' 'oops' 'operate' 'oru' 'pa' 'packing'\n",
      " 'pain' 'part' 'pass' 'password' 'patent' 'pay' 'peoples' 'per'\n",
      " 'performed' 'pick' 'pizza' 'place' 'plane' 'planning' 'play' 'please'\n",
      " 'pleased' 'pleasure' 'pls' 'po' 'pobox' 'poboxox36504w45wq' 'point'\n",
      " 'pounds' 'pours' 'press' 'prize' 'prob' 'promise' 'qjkgighjjgcbl'\n",
      " 'question' 'quick' 'rain' 'rate' 'rcv' 're' 'reached' 'real' 'realized'\n",
      " 'really' 'receive' 'receivea' 'recent' 'red' 'remember' 'reply'\n",
      " 'replying' 'representative' 'request' 'requests' 'respect' 'review'\n",
      " 'reward' 'ride' 'right' 'ringtone' 'rodger' 'room' 'roommate' 'roommates'\n",
      " 'rply' 'run' 'runs' 'said' 'sarcastic' 'saturday' 'save' 'saw' 'say'\n",
      " 'scotland' 'searching' 'second' 'see' 'seeing' 'seekers' 'seemed'\n",
      " 'sehwag' 'selected' 'send' 'series' 'seriously' 'service' 'set' 'sheets'\n",
      " 'sherawat' 'short' 'show' 'shower' 'shracomorsglsuplt' 'shy' 'sick'\n",
      " 'simply' 'since' 'situation' 'six' 'slice' 'smarter' 'smile' 'smiling'\n",
      " 'sms' 'smth' 'someone' 'something' 'song' 'soon' 'sooner' 'sorry' 'speak'\n",
      " 'special' 'spell' 'spent' 'spoilt' 'sptv' 'stand' 'started' 'starwars3'\n",
      " 'std' 'steed' 'still' 'stock' 'stop' 'str' 'stubborn' 'stuff'\n",
      " 'stuff42moro' 'subscription' 'sucker' 'suckers' 'sucks' 'sum1' 'sunday'\n",
      " 'suprman' 'sure' 'sweet' 'ta' 'take' 'talk' 'tb' 'tea' 'team' 'tell'\n",
      " 'telling' 'telugu' 'text' 'texting' 'thank' 'thanks' 'that' 'thats'\n",
      " 'think' 'thinked' 'tho' 'though' 'thought' 'tickets' 'till' 'time'\n",
      " 'times' 'timings' 'tkts' 'today' 'tomo' 'tomorrow' 'tonight' 'took'\n",
      " 'trav' 'treat' 'tried' 'trip' 'trouble' 'try' 'trying' 'tsandcs' 'turn'\n",
      " 'txt' 'tyler' 'uk' 'update' 'ur' 'urgent' 'urgnt' 'url' 'us' 'use' 'used'\n",
      " 'usf' 'usher' 'usually' 'vaguely' 'valid' 'valuable' 'valued' 've'\n",
      " 'verify' 'vettam' 'wah' 'wait' 'waiting' 'wales' 'wan' 'want' 'wanted'\n",
      " 'wap' 'wat' 'watching' 'watts' 'way' 'weak' 'week' 'weekend' 'well' 'wen'\n",
      " 'wet' 'whole' 'wif' 'win' 'wings' 'winner' 'wk' 'wkly' 'wo' 'wonderful'\n",
      " 'wont' 'word' 'words' 'work' 'world' 'worried' 'wow' 'wun' 'www' 'xuhui'\n",
      " 'xx' 'xxx' 'xxxmobilemovieclub' 'yeah' 'year' 'yes' 'yesterday' 'yo'\n",
      " 'yummy' 'yup' 'ú1']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "934"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A lot of duplicate tokens such as 'win'and 'winner'; 'reply' and 'replying'; 'want' and 'wanted' etc. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
